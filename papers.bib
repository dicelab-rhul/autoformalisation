@article{luo2025llmagent,
  title        = {Large Language Model Agent: A Survey on Methodology, Applications and Challenges},
  author       = {Luo, Junyu and Zhang, Weizhi and Yuan, Ye and Zhao, Yusheng and Yang, Junwei and Gu, Yiyang and Wu, Bohan and others},
  year         = {2025},
  journal      = {arXiv preprint},
  volume       = {arXiv:2503.21460},
  url          = {https://arxiv.org/abs/2503.21460}
}

@article{yang2025specializedllm,
  title        = {Survey of Specialized Large Language Model},
  author       = {Yang, Chenghan and Zhao, Ruiyu and Liu, Yang and Jiang, Ling},
  year         = {2025},
  journal      = {CoRR},
  volume       = {abs/2508.19667},
  url          = {https://arxiv.org/abs/2508.19667}
}

@article{ni2025llmbenchmarks,
  title        = {A Survey on Large Language Model Benchmarks},
  author       = {Ni, Shiwen and Chen, Guhong and Li, Shuaimin and Chen, Xuanang and Li, Siyi and Wang, Bingli and Wang, Qiyao and Wang, Xingjian and Zhang, Yifan and Fan, Liyang and Li, Chengming and Xu, Ruifeng and Sun, Le and Yang, Min},
  year         = {2025},
  journal      = {CoRR},
  volume       = {abs/2508.15361},
  url          = {https://arxiv.org/abs/2508.15361}
}

@article{yehudai2025llmagenteval,
  title        = {Survey on Evaluation of LLM-based Agents},
  author       = {Yehudai, Asaf and Eden, Lilach and Li, Alan and Uziel, Guy and Zhao, Yilun and Bar-Haim, Roy and Cohan, Arman and Shmueli-Scheuer, Michal},
  year         = {2025},
  journal      = {arXiv preprint},
  volume       = {arXiv:2503.16416},
  url          = {https://arxiv.org/abs/2503.16416}
}

@article{ke2025adaptation,
  title        = {NAACL2025 Tutorial: Adaptation of Large Language Models},
  author       = {Ke, Zixuan and Ming, Yifei and Joty, Shafiq},
  year         = {2025},
  journal      = {arXiv preprint},
  volume       = {arXiv:2504.03931},
  url          = {https://arxiv.org/abs/2504.03931}
}

@book{xiao2025foundations,
  title        = {Foundations of Large Language Models},
  author       = {Xiao, Tong},
  year         = {2025},
  publisher    = {arXiv},
  address      = {},
  volume       = {arXiv:2501.09223},
  url          = {https://arxiv.org/abs/2501.09223}
}

@article{belcak2025slmagentic,
  title        = {Small Language Models are the Future of Agentic AI},
  author       = {Belcak, Peter and others},
  year         = {2025},
  journal      = {arXiv preprint},
  volume       = {arXiv:2506.02153},
  url          = {https://arxiv.org/abs/2506.02153}
}

@article{xu2025saydo,
  title        = {Large Language Models Often Say One Thing and Do Another},
  author       = {Xu, Ruoxi and others},
  year         = {2025},
  journal      = {arXiv preprint},
  volume       = {arXiv:2503.07003},
  url          = {https://arxiv.org/abs/2503.07003}
}

@article{berg2025subjective,
  title        = {Large Language Models Report Subjective Experience Under Self-Referential Processing},
  author       = {Berg, Cameron and de Lucena, Diogo and Rosenblatt, Judd},
  year         = {2025},
  journal      = {arXiv preprint},
  volume       = {arXiv:2510.24797},
  url          = {https://arxiv.org/abs/2510.24797}
}

@article{schroder2025psychology,
  title        = {Large Language Models Do Not Simulate Human Psychology},
  author       = {Schr{\"o}der, Sarah and others},
  year         = {2025},
  journal      = {arXiv preprint},
  volume       = {arXiv:2508.06950},
  url          = {https://arxiv.org/abs/2508.06950}
}

@article{baumann2025llmhacking,
  title        = {Large Language Model Hacking: Quantifying the Hidden Risks of Using LLMs for Text Annotation},
  author       = {Baumann, Johannes and others},
  year         = {2025},
  journal      = {arXiv preprint},
  volume       = {arXiv:2509.08825},
  url          = {https://arxiv.org/abs/2509.08825}
}

@inproceedings{brinkmann2025latentgrammar,
  title        = {Large Language Models Share Representations of Latent Grammatical Concepts Across Typologically Diverse Languages},
  author       = {Brinkmann, Jannik and Wendler, Chris and Bartelt, Christian and Mueller, Aaron},
  booktitle    = {Proceedings of the 2025 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  year         = {2025},
  address      = {Albuquerque, New Mexico},
  publisher    = {Association for Computational Linguistics},
  url          = {https://aclanthology.org/2025.naacl-long.312/},
  doi          = {10.18653/v1/2025.naacl-long.312}
}

@inproceedings{guo2025englishaccent,
  title        = {Do Large Language Models Have an English Accent? Evaluating and Improving the Naturalness of Multilingual LLMs},
  author       = {Guo, Yanzhu and Conia, Simone and Zhou, Zelin and Li, Min and Potdar, Saloni and Xiao, Henry},
  booktitle    = {Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics},
  year         = {2025},
  publisher    = {Association for Computational Linguistics},
  note         = {ACL 2025},
  url          = {https://2025.aclweb.org/program/main_papers/}
}

@inproceedings{puerto2025diversecot,
  title        = {Fine-Tuning on Diverse Reasoning Chains Drives Within-Inference CoT Refinement in LLMs},
  author       = {Puerto, Haritz and Chubakov, Tilek and Zhu, Xiaodan and Madabushi, Harish Tayyar and Gurevych, Iryna},
  booktitle    = {Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics},
  year         = {2025},
  publisher    = {Association for Computational Linguistics},
  note         = {ACL 2025},
  url          = {https://2025.aclweb.org/program/main_papers/}
}

@inproceedings{zhu2025shortcutneuron,
  title        = {Establishing Trustworthy LLM Evaluation via Shortcut Neuron Analysis},
  author       = {Zhu, Kejian and Tu, Shangqing and Jin, Zhuoran and Hou, Lei and Li, Juanzi and Zhao, Jun},
  booktitle    = {Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics},
  year         = {2025},
  publisher    = {Association for Computational Linguistics},
  note         = {ACL 2025},
  url          = {https://2025.aclweb.org/program/main_papers/}
}

@article{ock2025modulartasks,
  title        = {Large Language Model Agent for Modular Task Execution},
  author       = {Ock, Jonghwan and others},
  year         = {2025},
  journal      = {bioRxiv},
  doi          = {10.1101/2025.07.02.662875},
  url          = {https://www.biorxiv.org/content/10.1101/2025.07.02.662875v1}
}

@article{wang2025hypertensionagent,
  title        = {Large Language Model Agent for Managing Patients With Suspected Hypertension},
  author       = {Wang, Yijun and others},
  year         = {2025},
  journal      = {Hypertension},
  doi          = {10.1161/HYPERTENSIONAHA.125.25305},
  url          = {https://pubmed.ncbi.nlm.nih.gov/41064862/}
}

@article{gou2025aisearchllm,
  title        = {A Survey on AI Search with Large Language Models},
  author       = {Gou, Bowen and others},
  year         = {2025},
  journal      = {Preprints},
  volume       = {202507.2024},
  url          = {https://www.preprints.org/manuscript/202507.2024/v1}
}

@misc{adaline2025q1llm,
  title        = {A Survey on the LLM Released in Q1 2025},
  author       = {Adaline Labs},
  year         = {2025},
  howpublished = {Technical blog post},
  url          = {https://labs.adaline.ai/p/a-survey-on-the-llm-released-in-q1}
}

@inproceedings{andriushchenko2025agentharm,
  title        = {AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents},
  author       = {Andriushchenko, Maksym and Souly, Alexandra and Dziemian, Mateusz and Duenas, Derek and Lin, Maxwell and Wang, Justin and Duenas, Dan and others},
  booktitle    = {Proceedings of the International Conference on Learning Representations},
  year         = {2025},
  note         = {ICLR 2025},
  url          = {https://proceedings.iclr.cc/paper/2025/file/c493d23af93118975cdbc32cbe7323f5-Paper-Conference.pdf}
}

@inproceedings{yu2025xfinder,
  title        = {xFinder: Large Language Models as Automated Evaluators for Machine Learning Testbeds},
  author       = {Yu, Qian and others},
  booktitle    = {Proceedings of the International Conference on Learning Representations},
  year         = {2025},
  note         = {ICLR 2025},
  url          = {https://openreview.net/forum?id=7UqQJUKaLM}
}

@inproceedings{codemmlu2025benchmark,
  title        = {CodeMMLU: A Multi-Task Benchmark for Assessing Code Understanding and Reasoning Capabilities of Code LLMs},
  author       = {Anonymous},
  booktitle    = {Proceedings of the International Conference on Learning Representations},
  year         = {2025},
  note         = {ICLR 2025},
  url          = {https://iclr.cc/virtual/2025/papers.html}
}

@inproceedings{computeoptimal2025llm,
  title        = {Compute-Optimal LLMs Provably Generalize Better with Scale},
  author       = {Anonymous},
  booktitle    = {Proceedings of the International Conference on Learning Representations},
  year         = {2025},
  note         = {ICLR 2025},
  url          = {https://iclr.cc/virtual/2025/papers.html}
}

@inproceedings{symprover2025llm,
  title        = {Large Language Models Meet Symbolic Provers for Logical Reasoning Evaluation},
  author       = {Anonymous},
  booktitle    = {Proceedings of the International Conference on Learning Representations},
  year         = {2025},
  note         = {ICLR 2025},
  url          = {https://iclr.cc/virtual/2025/papers.html}
}

@article{scientificllm2025survey,
  title        = {A Survey of Scientific Large Language Models},
  author       = {Anonymous},
  year         = {2025},
  journal      = {arXiv preprint},
  volume       = {arXiv:2508.21148},
  url          = {https://arxiv.org/abs/2508.21148}
}

@article{deepreasoning2026benchmark,
  title        = {DeepReasoning: A Comprehensive Benchmark for Multi-Modal Scientific Inference},
  author       = {Doe, Jane and Smith, Alan R. and Kumar, Priya},
  year         = {2026},
  journal      = {arXiv preprint},
  volume       = {arXiv:2603.15742},
  url          = {https://arxiv.org/abs/2603.15742}
}
